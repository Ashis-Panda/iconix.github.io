---
layout: post
section-type: post
title: Title
category: Category
tags: [ 'tag1', 'tag2' ]
---

Idea 5. RMSprop(lr=0.1) => Adam(lr=0.001)?
Got this idea after looking at the code behind vgg.finetune in lesson 1. That uses Adam instead of RMSprop. This provided the most promising boost so far.

Iteration #3 (use Adam(lr=0.001) optimizer) results:
Epoch 1/3
1500/1500 [==============================] - 60s - loss: 3.8602 - acc: 0.1427 - val_loss: 2.1919 - val_acc: 0.3040
Epoch 2/3
1500/1500 [==============================] - 60s - loss: 2.8074 - acc: 0.2807 - val_loss: 2.0083 - val_acc: 0.3390
Epoch 3/3
1500/1500 [==============================] - 61s - loss: 2.2858 - acc: 0.3767 - val_loss: 2.0740 - val_acc: 0.3570

This significant performance improvement is pretty frustrating, as differences between the various optimizers used for gradient descent have not been covered yet. I was using RMSprop like the lesson2.ipynb notebook from the course materials, on the faith that it is a good default. Perhaps my mistake was assuming that because it's a good default for cats vs dogs, it translates to this competition... but there's a lot of assuming going on so early in this course right now, so how to choose which assumptions to question? Feels like a big downside of top-down learning.