---
layout: post
section-type: post
title: First Kaggle Competition
category: Portfolio Building
tags: [ ]
---


Earlier this month, I listened to a [podcast](https://hanselminutes.com/580/machine-learning-deep-learning-and-artificial-intelligence-with-edaena-salinas-jasso) that introduced me to [fast.ai](http://www.fast.ai/) [^fast-ai-blog]. The tongue-in-cheek objective of fast.ai is "Making Neural Nets Uncool Again" by making deep learning more accessible and less exclusively the domain of research and PhDs.

I like their [teaching philosophy](http://www.fast.ai/2016/10/08/teaching-philosophy/) and avid claim that deep learning (and _good education_ in general) needn't be that complicated. So I've started to take their [Practical Deep Learning for Coders, Part 1](http://course.fast.ai/) course.

Rachel Thomas and Jeremy Howard are the team behind fast.ai, with Jeremy having previous experience as President and Chief Scientist of [Kaggle](https://www.kaggle.com/), a data science and machine learning competition site. So it shouldn't be surprising that Kaggle has a prominent role in the course.

Kaggle has been on the periphery of my awareness ever since receiving [yet another recommendation](/blogging/2017/05/07/hello-world.html#building-a-portfolio) from my data-science-over-coffee mentor that fine Sunday morning in April. She strongly encouraged me to try my hand at past Kaggle competitions - past competitions because she felt they would be a useful platform for learning, but with less competitive stress. So I jotted that down as a critical to-do.

Fast-forward to taking this course, and my true introduction to Kaggle is now underway. For my _very first_ homework assignment, I entered the [State Farm Distracted Driver Detection](https://www.kaggle.com/c/state-farm-distracted-driver-detection) competition from August 2016[^notebook]. The fact that I could do something like this in Week 1 (granted, with heavy assistance from the course material) is what makes this course so special. Providing these practical skills from the onset is core to the fast.ai philosophy, and I'm enjoying it so far.

Like Jeremy has stressed in lecture, learning in this way takes a leap of faith. I do not understand much of the underlying mechanisms for why my Kaggle score is so competitive (an "ok-ish" top 45% with minimal tweaking on my part from what was taught in class). But I trust that over the remaining 6 weeks of lecture, we will fully plumb the depths of what deep learning can be. And after seeing the view from the top, I'm excited to take that leap[^rnn].

#### Footnotes
[^fast-ai-blog]: [Previous blog post](/blogging/2017/07/09/fast-ai-blog.html) about my intro to fast.ai.
[^notebook]: I placed [my Jupyter notebook](https://github.com/iconix/fast.ai/blob/master/nbs/lesson1-hmwk.ipynb) for this assignment/competition on GitHub.
[^rnn]: I'm also a _little_ impatient to get to [Week 5 NLP + Recurrent Neural Networks](http://course.fast.ai/lessons/lesson5.html), which I feel will have the most immediate benefit to my day job in the Notes world.
